# Tic-Tac-Toe AI - Agentes Inteligentes para o Jogo da Velha

Este projeto apresenta uma implementa√ß√£o completa do Jogo da Velha, destacando a cria√ß√£o e o treinamento de agentes de Intelig√™ncia Artificial. A aplica√ß√£o possui uma interface gr√°fica desenvolvida com Tkinter e permite que os usu√°rios joguem contra diferentes tipos de IA ou observem as IAs jogando entre si.

O foco principal √© a demonstra√ß√£o e compara√ß√£o de duas abordagens cl√°ssicas de IA para jogos de tabuleiro:

1.  **Agente Minimax**: Uma IA determin√≠stica que utiliza o algoritmo Minimax para realizar buscas exaustivas no espa√ßo de estados do jogo. O resultado √© um agente que joga de forma "perfeita", sendo imposs√≠vel venc√™-lo (o melhor resultado poss√≠vel para um oponente √© o empate).

2.  **Agente Q-Learning**: Uma IA baseada em Aprendizado por Refor√ßo. Este agente n√£o possui conhecimento pr√©vio das regras e aprende a jogar atrav√©s de milhares de partidas de treinamento. Ele utiliza uma Tabela Q para associar o valor de cada a√ß√£o a cada estado do jogo, refinando sua estrat√©gia com base em recompensas e puni√ß√µes.

O projeto √© dividido em um modo de **treinamento**, executado via linha de comando, e um modo de **jogo**, com interface gr√°fica.

## üöÄ Funcionalidades

  * **L√≥gica de Jogo Encapsulada**: As regras do Jogo da Velha s√£o isoladas em sua pr√≥pria classe, permitindo f√°cil manuten√ß√£o e reutiliza√ß√£o.
  * **M√∫ltiplos Agentes**: Jogue como humano ou escolha entre agentes: Aleat√≥rio, Minimax (imbat√≠vel) ou Q-Learning (aprendiz).
  * **Treinamento Configur√°vel**: Treine o agente Q-Learning contra um oponente aleat√≥rio ou o poderoso Minimax para refinar sua Tabela Q.
  * **Persist√™ncia de Conhecimento**: A Tabela Q do agente √© salva em um arquivo `q_table.json`, permitindo que o aprendizado seja retomado a qualquer momento.
  * **Visualiza√ß√£o do Treinamento**: Um script separado permite plotar gr√°ficos em tempo real, mostrando a evolu√ß√£o da performance do agente (taxa de vit√≥rias, empates, derrotas) e a varia√ß√£o do `epsilon` ao longo dos epis√≥dios.
  * **Interface Gr√°fica Intuitiva**: Uma UI simples constru√≠da com Tkinter para configurar e jogar as partidas.

## üõ†Ô∏è Estrutura do Projeto

O c√≥digo √© organizado de forma modular para separar as responsabilidades:

  - `game/`: Cont√©m a l√≥gica fundamental do jogo (`tic_tac_toe.py`).
  - `agents/`: Cont√©m as implementa√ß√µes dos agentes de IA (`minimax_agent.py`, `qlearning_agent.py`).
  - `ui_tk/`: Cont√©m todos os componentes da interface gr√°fica.
  - `history/`: Pasta onde o hist√≥rico de treinamento √© salvo (`history.csv`).
  - `main.py`: Ponto de entrada que direciona para o modo de jogo ou treinamento.
  - `training_manager.py`: Orquestra as sess√µes de treinamento.
  - `plot_live.py`: Script para visualiza√ß√£o do progresso do treinamento.

## ‚öôÔ∏è Instala√ß√£o e Execu√ß√£o

Para executar o projeto, voc√™ precisar√° do Python 3. √â altamente recomend√°vel usar um ambiente virtual.

1.  **Clone o reposit√≥rio:**

    ```bash
    git clone https://github.com/sarpa-m/tic_tac_toe_ai.git
    cd tic_tac_toe_ai
    ```

2.  **Crie e ative um ambiente virtual:**

    ```bash
    # Para Windows
    python -m venv .venv
    .venv\Scripts\activate

    # Para macOS/Linux
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Instale as depend√™ncias a partir do `requirements.txt`:**

    ```bash
    pip install -r requirements.txt
    ```

## üéÆ Como Jogar

Para iniciar a interface gr√°fica e jogar, basta executar o `main.py` sem argumentos:

```bash
python main.py
```

Na tela do jogo, voc√™ pode:

  - Selecionar o tipo de jogador para 'X' e 'O' (Humano, Minimax, Q-Learning, Aleat√≥rio).
  - Definir o n√∫mero de partidas a serem jogadas em sequ√™ncia.
  - Ajustar a velocidade (atraso em milissegundos) entre as jogadas das IAs.

## üß† Treinando a IA

Para treinar o agente Q-Learning, utilize a flag `--train` na linha de comando. Voc√™ pode configurar o oponente, o n√∫mero de epis√≥dios e o s√≠mbolo do agente.

**Exemplo 1: Treinar por 10.000 epis√≥dios contra um oponente aleat√≥rio.**

```bash
python main.py --train --episodes 10000 --opponent random
```

**Exemplo 2: Treinamento avan√ßado por 5.000 epis√≥dios contra o agente Minimax.**

```bash
python main.py --train --episodes 5000 --opponent minimax
```

### Visualiza√ß√£o em Tempo Real

Enquanto o treinamento est√° em execu√ß√£o em um terminal, voc√™ pode abrir **um segundo terminal** (com o mesmo ambiente virtual ativado) e executar o `plot_live.py` para ver o progresso:

```bash
python plot_live.py
```

Isso abrir√° uma janela do Matplotlib que se atualiza automaticamente, mostrando a taxa de vit√≥rias, empates, derrotas e o decaimento do `epsilon`.

## üí° Destaques do C√≥digo



### L√≥gica do Jogo: `check_winner()`

A verifica√ß√£o de vit√≥ria √© feita de forma sistem√°tica, checando todas as 8 combina√ß√µes poss√≠veis.

```python
# Em game/tic_tac_toe.py
def check_winner(self):
    """Verifica o estado do jogo."""
    wins = [
        (0, 1, 2), (3, 4, 5), (6, 7, 8),  # linhas
        (0, 3, 6), (1, 4, 7), (2, 5, 8),  # colunas
        (0, 4, 8), (2, 4, 6)              # diagonais
    ]
    for i, j, k in wins:
        if self.board[i] and self.board[i] == self.board[j] == self.board[k]:
            return self.board[i]

    if self.is_full():
        return 'Draw'

    return None
```

### Agente Minimax: A Recurs√£o

O n√∫cleo do Minimax √© uma fun√ß√£o recursiva que alterna entre maximizar a pontua√ß√£o da IA e minimizar a pontua√ß√£o do oponente.

```python
# Em agents/minimax_agent.py
def minimax(self, board: TicTacToe, depth: int, is_maximizing: bool) -> int:
    """Fun√ß√£o recursiva Minimax."""
    score = self.evaluate(board)
    if score is not None:
        return score

    if is_maximizing:
        best_val = -math.inf
        for move in board.get_available_moves():
            board.make_move(move, self.ai_player)
            val = self.minimax(board, depth + 1, False)
            board.board[move] = None  # desfaz movimento
            best_val = max(best_val, val)
        return best_val
    else:
        best_val = math.inf
        for move in board.get_available_moves():
            board.make_move(move, self.human_player)
            val = self.minimax(board, depth + 1, True)
            board.board[move] = None  # desfaz movimento
            best_val = min(best_val, val)
        return best_val
```

### Agente Q-Learning: A Equa√ß√£o de Aprendizado

A atualiza√ß√£o da Tabela Q √© feita usando a equa√ß√£o de Bellman, que combina a recompensa imediata com a melhor recompensa futura esperada.

```python
# Em agents/qlearning_agent.py
def update_q(self, state: tuple, action: int, reward: float, next_state: tuple, next_moves: list):
    key = self._state_key(state)
    # ...
    old_q = self.q_table[key].get(str(action), 0.0)
    future_q = 0.0
    if next_moves:
        future_q = max(self.get_q(next_state, a) for a in next_moves)
    
    # Equa√ß√£o de Bellman
    new_q = old_q + self.alpha * (reward + self.gamma * future_q - old_q)
    self.q_table[key][str(action)] = new_q
```

### Gerenciador de Treinamento: O Loop

O `TrainingManager` orquestra os epis√≥dios de treinamento, alternando as jogadas e acionando o aprendizado do agente.

```python
# Em training_manager.py
def _train_vs_random(self):
    # ...
    for ep in tqdm(range(1, self.num_episodes + 1), ...):
        game = TicTacToe()
        state = game.get_state()
        done = False

        while not done:
            # Jogada do agente
            move = self.agent.choose_action(state, game.get_available_moves())
            game.make_move(move, self.agent_symbol)
            # ... verifica fim de jogo ...

            # Jogada aleat√≥ria do oponente
            opp = random.choice(game.get_available_moves())
            game.make_move(opp, self.opponent_symbol)
            
            # Agente aprende com o resultado da rodada
            ns2 = game.get_state()
            self.agent.update_q(state, move, 0.0, ns2, game.get_available_moves())
            state = ns2
```
